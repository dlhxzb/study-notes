# 分布式系统课程学习笔记
课程很nice，凝缩一下方便日后回味
https://night-cruise.github.io/mit-6.824/index.html  

**TODO:** 写一个Raft crate练练手
  
- [分布式系统课程学习笔记](#分布式系统课程学习笔记)
  - [1. 简介](#1-简介)
    - [可扩展性](#可扩展性)
    - [可用性](#可用性)
        - [可恢复性](#可恢复性)
    - [一致性](#一致性)
    - [MapReduce](#mapreduce)
  - [2. GFS(Google File System)](#2-gfsgoogle-file-system)
    - [难点](#难点)
    - [错误的设计](#错误的设计)
    - [物理架构](#物理架构)
    - [读时序](#读时序)
    - [写时序](#写时序)
  - [3. VMware FT](#3-vmware-ft)
    - [复制的两种方法](#复制的两种方法)
    - [流程](#流程)
    - [重复输出](#重复输出)
    - [主从脑裂](#主从脑裂)
  - [4. Raft](#4-raft)
    - [脑裂](#脑裂)
    - [过半票决](#过半票决)
    - [Log时序](#log时序)
    - [日志（Raft Log）](#日志raft-log)
    - [Leader选举](#leader选举)
    - [选举定时器](#选举定时器)
    - [日志恢复](#日志恢复)
    - [选举约束](#选举约束)
    - [快速恢复](#快速恢复)
    - [持久化](#持久化)
    - [日志快照](#日志快照)
  - [Zookeeper](#zookeeper)
    - [线性一致（强一致）](#线性一致强一致)
    - [Zookeeper架构](#zookeeper架构)
    - [一致保证](#一致保证)
    - [sync](#sync)
    - [就绪文件（Ready file）](#就绪文件ready-file)
    - [使用Zookeeper实现读写原子](#使用zookeeper实现读写原子)
    - [使用Zookeeper实现非扩展锁](#使用zookeeper实现非扩展锁)
    - [使用Zookeeper实现可扩展所](#使用zookeeper实现可扩展所)
    - [链复制](#链复制)
    - [链复制的故障恢复（Fail Recover）](#链复制的故障恢复fail-recover)
    - [链复制的配置管理器](#链复制的配置管理器)
  - [Aurora, 云复制DB](#aurora-云复制db)
    - [事务的流程](#事务的流程)
    - [关系型数据库（Amazon RDS）](#关系型数据库amazon-rds)
    - [Quorum 复制机制](#quorum-复制机制)
    - [Aurora读写存储服务器](#aurora读写存储服务器)
    - [数据分片](#数据分片)
    - [只读数据库](#只读数据库)
      - [B-Tree rebalance \& 微事务](#b-tree-rebalance--微事务)
    - [Aurora总结](#aurora总结)
  - [缓存一致性：Frangipani](#缓存一致性frangipani)
    - [锁服务器](#锁服务器)
    - [缓存一致性](#缓存一致性)
    - [原子性](#原子性)
    - [WAL](#wal)
    - [故障恢复](#故障恢复)
  - [分布式事务](#分布式事务)
    - [并发控制](#并发控制)
      - [悲观并发控制（Pessimistic Concurrency Control）](#悲观并发控制pessimistic-concurrency-control)
      - [乐观并发控制（Optimistic Concurrency Control）](#乐观并发控制optimistic-concurrency-control)
    - [二阶段提交（原子提交）](#二阶段提交原子提交)
    - [故障恢复](#故障恢复-1)
    - [总结](#总结)


## 1. 简介
基础架构的类型主要是存储，通信（网络）和计算，课程主要关注存储，因为这是一个定义明确且有用的抽象概念，并且通常比较直观。
`CAP`：
* 一致性 (Consistency)：写入后读出的一定都是新值
* 可用性 (Availability)：高可用性代表请求能够及时处理，不会一直等待，即使出现节点失效
* 分区容错性 (Partition tolerance)：脑裂仍能正常对外提供服务。
### 可扩展性
通过增加计算机来增加系统性能或吞吐量，分布式存储存在的意义
自身项目：按开户地将redis分为10个组，每组存储不同的数据，组内两台机器数据冗余
### 可用性
即容错。大型分布式系统中出故障的几率被放大，集群中总会有故障。通常通过数据冗余来实现。  
主要实现手段：复制，难点在于避免多副本间偏离同步状态。  
自身项目：自研edisync管理两个redis，写时用双写，牺牲性能来满足强一致性要求
##### 可恢复性
比可用性更弱的需求，有人修复后系统可以继续之前未完成的工作。  
主要实现手段：非易失存储，记录checkpoint、log。比如LSM数据库的WAL
自身项目：自研resumable记录checkpoint
### 一致性
写入后读出的一定都是新值，无论从哪个副本。有时为了性能会妥协为最终一致性  
自身项目：金融系统要求强一致，redis双写，失败的标记为不可用，等待人为介入

### MapReduce
框架会为每个输入文件运行Map函数，输出KV，第二阶段是运行Reduce函数

## 2. GFS(Google File System)
### 难点
为了性能，人们设计分布式系统，使用大量计算机同时完成工作，数据也需要分割到大量服务器上。  
大量服务器带来的问题是总会有服务器故障，引出了容错（可用性）需求。最有用的一种方法是使用复制，维护2-3个数据的副本。然而一不小心，副本就会不一致，通过设计可以避免，但是会牺牲性能，好的设计是一种平衡  
提高性能 -> 分片 -> 故障 -> 容错 -> 复制 -> 一致性 -> 降低性能
### 错误的设计
请求按不同顺序到达副本，执行结果不一致。  
自身项目：我们将客户自身请求排队，多客户之间并发，单客户内部串行，保证客户操作一致性
### 物理架构
* Master server：热备，同时激活的只有单点
* Chunk server：每个数据块由三个服务器冗余存储，由master定位读取
### 读时序
* 1. Master接客户端请求，从自身表单查Chunk服务器列表，将其返回客户端
* 2. 客户端选择最近的Chunk（Google的数据中心中，IP地址是连续的）读取数据
### 写时序
* 1. 客户端追加数据向Master请求Chunk位置
* 2. Master找到Chunk的主副本，如不存在Master根据版本号找出所有存有Chunk最新副本的Chunk服务器，挑选一个作为Primary，其他的作为Secondary
* 3. Master会增加版本号，并将版本号写入磁盘
* 4. Master通知这些服务器谁是Primary谁是Secondary
* 5. 客户端向Primary写入，Primary向Secondary同步，所有都同步成功后Primary向客户端返回成功，否则客户端从原来的Chunk位置重新写入
* 6. Primary身份租约只有60s，以此来保证不会同时有两个Primary
  
## 3. VMware FT
### 复制的两种方法
* 状态转移：每过一会进行内存拷贝
* 复制状态机：以相同的顺序执行外部输入
后者更为普遍，传输数据小，但要考虑的也更复杂
### 流程
* 1. Primary所在机器的VMM(虚拟机监控机)接收到客户请求，VMM将请求发送给Primary虚拟机，同时复制给Backup虚拟机所在的VMM
* 2. Backup VMM收到请求后向Primary VVM回复ACK
* 3. Primary将结果回复给VMM，VMM只有收到上面ACK后才回复客户端，确保每个请求都到达了Backup
* 4. Backup将结果回复给自己的VMM，VMM知道自己是Backup，丢去报文
Backup检测到Primary故障后，向客户端请求顶替Primary  
对于`3.`几乎每一个复制系统都有这个问题，在某个时间点，Primary必须要停下来等待Backup
### 重复输出
对于任何有主从切换的复制系统，基本上不可能将系统设计成不产生重复输出。这里我们使用的是TCP来完成重复检测。  
自身项目：自研resumable记录请求key，重复的直接过滤掉
### 主从脑裂
向一个外部的第三方权威机构求证，来决定Primary还是Backup允许上线。这里的第三方就是Test-and-Set服务。  
Backup上线需要像它申请。

## 4. Raft
### 脑裂
上面的Test-and-Set服务的缺点是可能会单点故障，若采用多Test-and-Set副本，则可能因为脑裂产生多Primary。  
### 过半票决
奇数台服务器服务器进行过半投票，可以保留脑裂后的大半区，舍弃少数  
Raft的操作需要过半服务器完成才算完成，所以新旧Leader的Follower中至少有一台是重叠的，新的Leader必然知道旧Leader使用的任期号（term number）。
### Log时序
Raft层封装在应用程序之下，读请求Leader的应用程序直接处理，写请求：
* 1. 广播： Leader应用程序将请求(Log)下发到Raft，Raft节点之间相互交互
* 2. 过半ACK：过半的Raft节点将这个新的操作加入到它们的日志并通知Leader收到请求
* 3. 执行：Leader Raft知道过半节点记录了请求后，通知应用程序执行操作，并通知其它Follower执行
第3步的Follower执行不需要返回值，可能会在log队列里堆积
### 日志（Raft Log）
所有节点都保存log，log有几个作用：
* Log是Leader用来对操作排序的一种手段，对于这些复制状态机来说至关重要
* Follower来说，Log是用来存放临时操作的地方，不确定是否commit
* Leader用log来给缺失的Follower补发请求
* 故障恢复的机器重新加入集群时，从磁盘读取log重建故障前的状态
### Leader选举
Raft使用任期号（term number）来区分不同的Leader。Followers不需要知道Leader的ID，它们只需要知道当前的任期号。  
每个节点都有一个选举定时器，如果在这个定时器耗尽之前，当前节点没有收到任何当前Leader的消息，这个节点会认为Leader已经下线，并开始一次选举。  
当前服务器会增加任期号，发出请求投票，过半当选。每一个节点在一个任期内只会投一票。赢得选举后心跳通知其它节点。  
如果旧Leader在少数分区，那么客户端的请求永远得不到过半ACK，也就永远不会响应
### 选举定时器
同时选举可能因为选票割裂导致没有过半Leader产生，通过设置随机的选举定时器错开选举时间。（不能小于心跳时间）
### 日志恢复
AppendEntries消息中包含prevLogIndex和prevLogTerm。
新Leader当选后发出AppendEntries，可能与Followers自己的最后一条log不符，会遭到拒绝。  
Leader为每个节点维护nextIndex，被拒绝后nextIndex-1，直到prevLog与节相符，发送其后所有log替换节点本地log
### 选举约束
* 候选人最后一条Log条目的任期号大于本地最后一条Log条目的任期号；
* 或者，候选人最后一条Log条目的任期号等于本地最后一条Log条目的任期号，且候选人的Log记录长度大于等于本地Log记录的长度
### 快速恢复
对于离线时间长的节点，恢复时Leader可以直接以任期号为单位向前跳跃，寻找恢复开始位置
### 持久化
只有三个数据需要持久化：
* Log：这是唯一记录了应用程序状态的地方
* currentTerm：确保每个任期只有最多一个Leader
* votedFor：同上
write函数返回时，并不能确保数据存在磁盘上，需要调用fsync，为了性能可以积攒大量客户端请求log，一次写入再发送AppendEntries
### 日志快照
对于一个长期运行的系统，log大量增长，如果重启后从头开始执行log重建状态花费太大。  
Raft快照（Snapshots）将应用程序状态拷贝，作为一种特殊的Log条目存储下来。快照对应槽位号之前的这部分Log可以被丢弃。  
所以，Raft的持久化存储实际上是持久化应用程序快照，和快照之后的Log

## Zookeeper
### 线性一致（强一致）
意味着集群表现的像一个服务器。  
对集群来说
* 执行顺序与请求时间顺序一致
* 写请求之后立即读一定能读出新值
要求对于集群内各节点来说，写请求顺序都一致。  
线性一致的历史记录必须与实际执行顺序一致。 
对于故障重发的请求，如果被处理过则跳过或直接返结果。
自身项目：resumable重复请求直接返结果
### Zookeeper架构
Leader需要与所有Follower通信，越增加机器越使得Leader成为瓶颈。可以分流读写请求，让读直接访问Follower。  
那么如何保证Follower读出不是旧数据，自身不是与Leader失联呢？  
Zookeeper的方式是，放弃读请求线性一致性，允许提供旧数据。。。写请求是线性一致的。
### 一致保证
* 任何一个客户端的请求，都会按照客户端指定的顺序来执行，FIFO
* 请返回包含log的zxid，下一个请求会带上它，Follower在自身log>=zxid之前不会处理
自身项目：Zookeeper以客户端为单位序列化请求，保证一致性，这与我们用ert以用户为单位序列化请求原理一致
### sync
Zookeeper有一个操作类型是sync，它本质上就是一个写请求，会出现在所有副本的Log中。  
写->sync->读，保证Follower在看到sync之前不处理读。  
但是sync会经由Leader，得必须不要这样做
### 就绪文件（Ready file）
假设有另外一个分布式系统，这个分布式有一个Master节点，而Master节点在Zookeeper中维护了一个配置，这个配置对应了一些file。  
那么如何对这些配置文件原子更新呢？  
假设有一些以Ready为名字的file，如果Ready file存在，那么允许读这个配置。（类似于锁）  
更新操作：delete ready -> write file1 -> write file2 -> create ready  
客户端通过调用exist来判断Ready file是否存在，并会一直watch它，中途被删除会等到创建后重读file
### 使用Zookeeper实现读写原子
get出数据带版本号，set时指定版本，实现mini-transaction
### 使用Zookeeper实现非扩展锁
创建文件：上锁，删除：释放锁，已存在：watch，等释放。  
这个设计不好，与上一个原子读写一样存在羊群效应，每一次释放通知所有watch再次尝试
### 使用Zookeeper实现可扩展所
为避免羊群效应，不再使用一个锁文件，而是创建Sequential文件，仅watch比我们Sequential小一个的文件
### 链复制
Chain Replication是与Raft不同的解决方案
* 写请求发给head，链式一个个传递到tail，全体commit，tail回复客户端写完成。
* 读请求发给tail
### 链复制的故障恢复（Fail Recover）
* Head故障：下一个节点成Head
* Tail故障：前一个节点成Tail
* 中间故障：移除
  
与Raft相比
* 写请求：链复制的Head直连下一个，比Leader负荷低，并发瓶颈来的更晚
* 读请求：Raft Leader可以看到所有的请求，这边只有Tail能看到读请求
* 故障恢复：链复制要简单很多
### 链复制的配置管理器
由于它不能抵御网络分区，也不能抵御脑裂。在实际场景中，会有一个外部权威来决定谁活，即配置管理器。  
配置管理器监测节点存活性，组织链结构。它本身基于Raft或者Paxos，并会通告给所有参与者（包括客户端）整个链的信息

## Aurora, 云复制DB
它是基于开源MySQL打造的
### 事务的流程
对于通常DB来说，事务是通过对涉及到的每一份数据加锁来实现。  
在硬盘中，除了有数据之外，还有一个预写式日志（WAL）对于系统的容错性至关重要。  
commit之前先写WAL，e.g. x-10,y+10的log
* x旧数据是500，我要将它改成510。
* y旧数据是750，我要将它改成740。
* 一个Commit日志，表明事务的结束。
旧数据也在log中，故可以undo。  
**在Commit日志落盘之后，就可以回复客户端事务已完成，实际上此时修改还在cache中，并未落盘。**  
数据库写磁盘是一个lazy操作，会积攒很多更新，提升操作速度。在此之前crash可以根据WAL redo。
### 关系型数据库（Amazon RDS）
在写入时只要完成过半服务器，即可返回，同步时不用data page用log效率更高
### Quorum 复制机制
Aurora使用了Quorum这种思想
* N：副本数
* W：写操作确保副本数
* R：读请求使用副本数
Quorum系统要求，任意你要发送写请求的W个服务器，必须与任意接收读请求的R个服务器有重叠。R + W > N否则不容错  
在Quorum系统中每一次执行写请求都会将新数值与增加版本号绑定，读请求返回最高版本号的回复   
每一条数据都要版本号？？？  
  
增加W减小R可降低写性能与容错，提高读性能与容错。W=N时R=1，即写要所有服务器确认（不再容错），读可从任意一个读
自身项目：Edisync server写要求所有redis强一致，读可改为随机一个就行
### Aurora读写存储服务器
数据库服务器+多个存储服务器，Log需要编号所以数据库服务器只能有一个  
写请求从来不会覆盖任何数据，它的写请求只会在当前Log中追加条目，完成W个副本的Log即可回复客户端完成。  
实际上，在一个故障恢复过程中，事务需要按照顺序恢复，那么在Aurora确认一个事务之前，它必须等待Write Quorum确认之前所有已提交的事务，
之后再确认当前的事务，最后才能回复给客户端。也就是事务的确认要串行？？？  
 
写请求Log的内容不会立即更新page，等到需要读page时才执行。  

写请求：
* 数据库服务器将Log发送到存储服务器
* 存储服务器将Log添加到page cache的列表
读请求：
* 如果数据库服务器的cache中没有该page，向存储服务器发读请求
* 存储服务器才会将Log条目中的新数据更新到page，page落盘并返回给数据库服务器
* 存储服务器在自身cache中会删除page对应的Log列表，并更新cache中的page
### 数据分片
对大型数据库Aurora每10GB作为一组在6个存储服务器中保存副本，多个分组可使用不同服务器。  
对于Log则要先找到其数据对应的分组，再发送。  
如果一台有100个10G数据块的服务器挂掉，恢复策略并不是用一台新机器拷贝100个数据块，
而是随机挑选100台现有机器每个拷贝1块，从100个源拷到100个目的地
### 只读数据库
Aurora不仅有主数据库实例，同时多个数据库的副本。读请求可以分担到副本上。
自身项目：这与我们多个只读redis child用法一致
当客户端向只读数据库发送读请求，只读数据库需要弄清楚它需要哪些data page来处理这个读请求，之后直接从存储服务器读取这些data page，并不需要主数据库的介入。  
当然，只读数据库也需要更新自身的缓存，所以，Aurora的主数据库也会将它的Log的拷贝发送给每一个只读数据库。
#### B-Tree rebalance & 微事务
数据库背后的B-Tree结构非常复杂，可能会定期触发rebalance，只读数据库直接从存储服务器读取数据库的page，它可能会看到在rebalance过程中的B-Tree。  
数据库服务器可以通知存储服务器说，这部分复杂的Log序列只能以原子性向只读数据库展示，也就是要么全展示，要么不展示。这就是微事务（Mini-Transaction）和VDL。 
所以当一个只读数据库需要向存储服务器查看一个data page时，存储服务器会小心的，要么展示微事务之前的状态，要么展示微事务之后的状态，但是绝不会展示中间状态。
### Aurora总结
通常我们需要有好的隔离解耦来区分上层服务和底层的基础架构，即数据库和存储系统解耦，存储系统应该是非常通用的，并不会为某个特定的应用程序定制。
但是在Aurora面临的问题中，性能问题是非常严重的，它不得不通过模糊服务和底层基础架构的边界来获得35倍的性能提升，这是个巨大的成功。

## 缓存一致性：Frangipani
缓存一致性是指，如果我缓存了一些数据，之后你修改了实际数据但是并没有考虑我缓存中的数据，必须有一些额外的工作的存在，这样我的缓存才能与实际数据保持一致。  
Frangipani是一个网络文件系统，缓存分布在各个用户的工作站上。除了共享的存储服务Petal，所有的复杂的逻辑都在工作站中的Frangipani模块中。所以这是一个非常去中心化的设计。
挑战：
* 1.修改的缓存如何自动同步到其它缓存
* 2.不同主机缓存的操作互相干扰
* 3.崩溃后不影响其它缓存完整性，以及故障恢复
### 锁服务器
Frangipani的缓存一致性核心是由锁保证的。在锁服务器里面，有一个表单叫locks，假设每个文件都有一个锁，它可被工作站持有。  
假设锁是排他锁（实际应该读写锁）。在每个工作站，会记录跟踪它所持有的锁。  
工作站流程：
* 向锁服务器申请锁
* 向Petal请求数据
* 文件操作完成后，继续持有并Idle锁
* 当需要时写回Petal，释放锁
### 缓存一致性
申请锁是一个异步请求：
* A向锁服务器申请一个被B持有的锁
* 锁服务器向B发出Revoke消息
* 如果锁已Idle且数据已修改，B将缓存写回Petal（先写WAL再数据）
* B向锁服务器发送Release
* 锁服务器向A发送Grant
对于读写锁写操作要Revoke所有读锁。
### 原子性
为了让多步骤的操作具备原子性，Frangipani在内部实现了一个数据库风格的事务系统，并且是以锁为核心。并且，这是一个分布式事务系统。  
简单来说就是整个操作的过程中持有所有的锁都不释放。
### WAL
一个工作站持有锁，并且在一个复杂操作的过程中崩溃了。
不太好的处理方法：
* 释放所有锁：如果崩溃在Petal写回时，其它工作站可能读出错误数据
* 不释放锁：行为正确，但等待太久
解决办法：在工作站向Petal写入任何数据之前，先在Petal内自己的Log列表里写入WAL，可由其它工作站执行。  
与其他系统不同的是，一般都是以数据为列表的Log，这里是以工作站为列表。
另外Log一般都存在本地，而这里是存在Petal
### 故障恢复
Frangipani出于一些原因对锁使用了租约，当租约到期了，锁服务器会认定工作站已经崩溃了，之后它会开始恢复流程：
* 向Petal写入Log时：直接释放锁
* 向Petal写入修改的文件时：让其它工作站来恢复。并在恢复完成时释放锁。

## 分布式事务
人们通常将`并发控制`和`原子提交`放在一起，当做事务。可以理解为将一些操作作为一个整体，不会因为失败而被分割，也不会被其他活动看到中间状态。
ACID：
* Atomic，原子性：要么所有的写数据都完成了，要么没有写数据能完成。
* Consistent，一致性：事务前后数据库的完整性没有被破坏，也就是数据符合规则
* Isolated，隔离性：两个同时运行的事务，在事务结束前，不能看到彼此的更新（例如串行）
* Durable，持久化的：已提交的不会丢失
### 并发控制
#### 悲观并发控制（Pessimistic Concurrency Control）
如果锁有冲突，其它事务需要等待。这里的锁是两阶段锁（Two-Phase Locking），这是一种最常见的锁。
第一个阶段获取锁，第二个阶段是在事务结束前一直持有锁。
如果不到最后仅在修改完数据就释放锁，那么其它事务可能看见事务的中间状态。（比如x-10,y+10，可能看见改过的x未改的y）  
对于死锁数据库一般会Abort其中一个事务，撤回它所有操作。
#### 乐观并发控制（Optimistic Concurrency Control）
在事务最后的时候，检查如果有其他的事务同一时间修改了你关心的数据并造成了冲突，那么你必须要Abort当前事务，并重试。
如果冲突非常少，那么乐观并发控制可以更快，因为它完全避免了锁带来的性能损耗。
### 二阶段提交（原子提交）
对于数据被分割在多台机器上如何支持事务原子性？  
两阶段提交不仅被分布式数据库所使用，同时也被各种看起来不像是传统数据库的分布式系统所使用。
事务协调者TC，服务器A，B，事务x+1,y-1，事务ID（TID）123
* 1. TC -Put-> A,B
  * 1.1 A,B lock x,y
* 2. TC -Prepare-> A,B
  * 2.0 A,B `save Log`
  * 2.1 A,B -Yes-> TC 
  * 2.3 TC `save Log`
* 3. TC -Commit-> A,B
  * 3.1 A,B set x,y and unlock, remove Log
  * 3.2 A,B -ACK->TC
### 故障恢复
* B在2.1回复Yes之前崩溃，那么即使重启后B也不知道事务，再次收到Prepare会回复No，事务Abort。
* B在2.1回复Yes之后崩溃，TC会认为所有服务器都OK，将发出Commit。
A将执行它那部分事务，持久化，释放锁，因此我们必须保障B在恢复后也执行它那部分，即2.0 log落盘。  
* B在3.2ACK之前崩溃，已经处理完Commit，删除了事务Log，第二次再来Commit就变成它不知道的事务，直接回复ACK
* TC在3.Commit之前崩溃，A,B会向TC查询事务，TC恢复后不认识该事务，直接Abort
* TC在3.部分Commit时崩溃，恢复时根据2.3Log向所有参与者重发Commit。但此时A,B会一直持有锁，并阻塞其它事务，称`Block区间`
* B在2.1回复Yes丢包，为避免重试时A持有锁过久，TC直接Abort，B网络恢复后询问事务123，TC会告诉它已Abort

### 总结
两阶段提交缺点
* 多轮消息有大量交互
* 大量的写磁盘操作
* 存在Block区间，尽量不要在大系统之间使用
本质是一个Leader（事务协调者），将消息发送给Follower（事务参与者），Leader只能在收到了足够多Follower的回复之后才能继续执行。
这与Raft非常像，但是，这里协议的属性与Raft又非常的不一样。这两个协议解决的是完全不同的问题。  
Raft的意义在于，即使部分参与的服务器故障了或者不可达，系统仍然能工作。  
Raft能做到这一点是因为所有的服务器都在做相同的事情，所以我们不需要所有的服务器都参与，我们只需要过半服务器参与。  
然而两阶段提交，参与者完全没有在做相同的事情，每个参与者都在做事务中的不同部分，所有的参与者都必须完成自己那部分工作，这样事务才能结束，所以这里需要等待所有的参与者。  
所以，Raft通过复制可以不用每一个参与者都在线，而两阶段提交每个参与者都做了不同的工作，并且每个参与者的工作都必须完成，所以两阶段提交对于可用性没有任何帮助。  
Raft完全就是可用性，而两阶段提交完全不是高可用的，系统中的任何一个部分出错了，系统都有可能等待直到这个部分修复。  
两阶段提交的可用性非常低，因为任何一个部分崩溃都有可能阻止整个系统的运行。  
然而，是有可能结合这两种协议的，通过Raft或者Paxos或者其他协议，来复制两阶段提交协议里的每一个组成部分。每个TC、事务参与者都是一个Raft集群。  
这里很复杂，但是它展示了你可以结合两种思想来同时获得高可用和原子提交。