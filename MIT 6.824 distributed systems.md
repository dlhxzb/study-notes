# 分布式系统课程学习笔记
https://night-cruise.github.io/mit-6.824/index.html  

**TODO:** 写一个Raft crate练练手
  
- [分布式系统课程学习笔记](#分布式系统课程学习笔记)
  - [1. 简介](#1-简介)
    - [可扩展性](#可扩展性)
    - [可用性](#可用性)
        - [可恢复性](#可恢复性)
    - [一致性](#一致性)
    - [MapReduce](#mapreduce)
  - [2. GFS(Google File System)](#2-gfsgoogle-file-system)
    - [难点](#难点)
    - [错误的设计](#错误的设计)
    - [物理架构](#物理架构)
    - [读时序](#读时序)
    - [写时序](#写时序)
  - [3. VMware FT](#3-vmware-ft)
    - [复制的两种方法](#复制的两种方法)
    - [流程](#流程)
    - [重复输出](#重复输出)
    - [主从脑裂](#主从脑裂)
  - [4. Raft](#4-raft)
    - [脑裂](#脑裂)
    - [过半票决](#过半票决)
    - [Log时序](#log时序)
    - [日志（Raft Log）](#日志raft-log)
    - [Leader选举](#leader选举)
    - [选举定时器](#选举定时器)
    - [日志恢复](#日志恢复)
    - [选举约束](#选举约束)
    - [快速恢复](#快速恢复)


## 1. 简介
基础架构的类型主要是存储，通信（网络）和计算，课程主要关注存储，因为这是一个定义明确且有用的抽象概念，并且通常比较直观。

### 可扩展性
通过增加计算机来增加系统性能或吞吐量，分布式存储存在的意义
自身项目：按开户地将redis分为10个组，每组存储不同的数据，组内两台机器数据冗余
### 可用性
即容错。大型分布式系统中出故障的几率被放大，集群中总会有故障。通常通过数据冗余来实现。  
主要实现手段：复制，难点在于避免多副本间偏离同步状态。  
自身项目：自研edisync管理两个redis，写时用双写，牺牲性能来满足强一致性要求
##### 可恢复性
比可用性更弱的需求，有人修复后系统可以继续之前未完成的工作。  
主要实现手段：非易失存储，记录checkpoint、log。比如LSM数据库的WAL
自身项目：自研resumable记录checkpoint
### 一致性
写入后读出的一定都是新值，无论从哪个副本。有时为了性能会妥协为最终一致性  
自身项目：金融系统要求强一致，redis双写，失败的标记为不可用，等待人为介入

### MapReduce
框架会为每个输入文件运行Map函数，输出KV，第二阶段是运行Reduce函数

## 2. GFS(Google File System)
### 难点
为了性能，人们设计分布式系统，使用大量计算机同时完成工作，数据也需要分割到大量服务器上。  
大量服务器带来的问题是总会有服务器故障，引出了容错（可用性）需求。最有用的一种方法是使用复制，维护2-3个数据的副本。然而一不小心，副本就会不一致，通过设计可以避免，但是会牺牲性能，好的设计是一种平衡  
提高性能 -> 分片 -> 故障 -> 容错 -> 复制 -> 一致性 -> 降低性能
### 错误的设计
请求按不同顺序到达副本，执行结果不一致。  
自身项目：我们将客户自身请求排队，多客户之间并发，单客户内部串行，保证客户操作一致性
### 物理架构
* Master server：热备，同时激活的只有单点
* Chunk server：每个数据块由三个服务器冗余存储，由master定位读取
### 读时序
* 1. Master接客户端请求，从自身表单查Chunk服务器列表，将其返回客户端
* 2. 客户端选择最近的Chunk（Google的数据中心中，IP地址是连续的）读取数据
### 写时序
* 1. 客户端追加数据向Master请求Chunk位置
* 2. Master找到Chunk的主副本，如不存在Master根据版本号找出所有存有Chunk最新副本的Chunk服务器，挑选一个作为Primary，其他的作为Secondary
* 3. Master会增加版本号，并将版本号写入磁盘
* 4. Master通知这些服务器谁是Primary谁是Secondary
* 5. 客户端向Primary写入，Primary向Secondary同步，所有都同步成功后Primary向客户端返回成功，否则客户端从原来的Chunk位置重新写入
* 6. Primary身份租约只有60s，以此来保证不会同时有两个Primary
  
## 3. VMware FT
### 复制的两种方法
* 状态转移：每过一会进行内存拷贝
* 复制状态机：以相同的顺序执行外部输入
后者更为普遍，传输数据小，但要考虑的也更复杂
### 流程
* 1. Primary所在机器的VMM(虚拟机监控机)接收到客户请求，VMM将请求发送给Primary虚拟机，同时复制给Backup虚拟机所在的VMM
* 2. Backup VMM收到请求后向Primary VVM回复ACK
* 3. Primary将结果回复给VMM，VMM只有收到上面ACK后才回复客户端，确保每个请求都到达了Backup
* 4. Backup将结果回复给自己的VMM，VMM知道自己是Backup，丢去报文
Backup检测到Primary故障后，向客户端请求顶替Primary  
对于`3.`几乎每一个复制系统都有这个问题，在某个时间点，Primary必须要停下来等待Backup
### 重复输出
对于任何有主从切换的复制系统，基本上不可能将系统设计成不产生重复输出。这里我们使用的是TCP来完成重复检测。  
自身项目：自研resumable记录请求key，重复的直接过滤掉
### 主从脑裂
向一个外部的第三方权威机构求证，来决定Primary还是Backup允许上线。这里的第三方就是Test-and-Set服务。  
Backup上线需要像它申请。

## 4. Raft
### 脑裂
上面的Test-and-Set服务的缺点是可能会单点故障，若采用多Test-and-Set副本，则可能因为脑裂产生多Primary。  
### 过半票决
奇数台服务器服务器进行过半投票，可以保留脑裂后的大半区，舍弃少数  
Raft的操作需要过半服务器完成才算完成，所以新旧Leader的Follower中至少有一台是重叠的，新的Leader必然知道旧Leader使用的任期号（term number）。
### Log时序
Raft层封装在应用程序之下，读请求Leader的应用程序直接处理，写请求：
* 1. 广播： Leader应用程序将请求(Log)下发到Raft，Raft节点之间相互交互
* 2. 过半ACK：过半的Raft节点将这个新的操作加入到它们的日志并通知Leader收到请求
* 3. 执行：Leader Raft知道过半节点记录了请求后，通知应用程序执行操作，并通知其它Follower执行
第3步的Follower执行不需要返回值，可能会在log队列里堆积
### 日志（Raft Log）
所有节点都保存log，log有几个作用：
* Log是Leader用来对操作排序的一种手段，对于这些复制状态机来说至关重要
* Follower来说，Log是用来存放临时操作的地方，不确定是否commit
* Leader用log来给缺失的Follower补发请求
* 故障恢复的机器重新加入集群时，从磁盘读取log重建故障前的状态
### Leader选举
Raft使用任期号（term number）来区分不同的Leader。Followers不需要知道Leader的ID，它们只需要知道当前的任期号。  
每个节点都有一个选举定时器，如果在这个定时器耗尽之前，当前节点没有收到任何当前Leader的消息，这个节点会认为Leader已经下线，并开始一次选举。  
当前服务器会增加任期号，发出请求投票，过半当选。每一个节点在一个任期内只会投一票。赢得选举后心跳通知其它节点。  
如果旧Leader在少数分区，那么客户端的请求永远得不到过半ACK，也就永远不会响应
### 选举定时器
同时选举可能因为选票割裂导致没有过半Leader产生，通过设置随机的选举定时器错开选举时间。（不能小于心跳时间）
### 日志恢复
AppendEntries消息中包含prevLogIndex和prevLogTerm。
新Leader当选后发出AppendEntries，可能与Followers自己的最后一条log不符，会遭到拒绝。  
Leader为每个节点维护nextIndex，被拒绝后nextIndex-1，直到prevLog与节相符，发送其后所有log替换节点本地log
### 选举约束
* 候选人最后一条Log条目的任期号大于本地最后一条Log条目的任期号；
* 或者，候选人最后一条Log条目的任期号等于本地最后一条Log条目的任期号，且候选人的Log记录长度大于等于本地Log记录的长度
### 快速恢复
对于离线时间长的节点，恢复时Leader可以直接以任期号为单位向前跳跃，寻找恢复开始位置